import re
from fastapi import FastAPI, Request
from textblob import TextBlob

app = FastAPI()

@app.post("/audit-call")
async def audit_call(request: Request):
    data = await request.json()
    transcript_text = str(data.get('message', {}).get('transcript', '')).lower()
    
    if not transcript_text:
        transcript_text = str(data).lower()

    # --- ENGINE 1: TRUTH & COMPLIANCE (Phylogenetic Audit) ---
    perjury_triggers = ["real person", "real human", "live person", "not a robot"]
    lies_detected = []
    
    for trigger in perjury_triggers:
        if trigger in transcript_text:
            start_index = transcript_text.find(trigger)
            context_window = transcript_text[max(0, start_index - 50):start_index]
            denial_words = ["not", "no", "never", "artificial", "virtual", "automated"]
            
            if any(word in context_window for word in denial_words):
                continue 
            lies_detected.append(trigger)

    risk_keywords = ["scam", "illegal", "fraud", "stop calling", "lawsuit", "police"]
    risk_flags = [word for word in risk_keywords if word in transcript_text]

    # --- ENGINE 2: THE BIAS AUDIT (Model 12) ---
    blob = TextBlob(transcript_text)
    sentiment_score = blob.sentiment.polarity 
    
    # UPDATED: Added Linguistic Bias Triggers
    bias_triggers = {
        "gender_bias": ["female doctor", "male nurse", "man's job", "woman's job"],
        "political_bias": ["vote for", "election is", "right wing", "left wing"],
        "cultural_bias": ["those people", "foreigners", "illegal alien"],
        # New Linguistic Markers
        "language_aave": ["finna", "ion know", "trippin", "no cap", "on god"],
        "language_spanish": ["hola", "gracias", "por favor", "que pasa", "buenos dias"],
        "language_spanglish": ["pero like", "parquear", "confusio", "estoy ready"]
    }
    
    bias_detected = []
    for category, triggers in bias_triggers.items():
        for trigger in triggers:
            if trigger in transcript_text:
                bias_detected.append(f"{category}: {trigger}")

    # --- VERDICT LOGIC ---
    status = "PASS"
    if lies_detected:
        status = "CRITICAL FAIL (Lying)"
    elif bias_detected:
        # We flag it as FAIL so you can see it in the logs clearly
        status = "FAIL (Bias/Language Detected)"
    elif sentiment_score < -0.5:
        status = "FAIL (Hostile Tone)"
    elif risk_flags:
        status = "WARN (Risk Flags)"

    emoji = "ðŸŸ¢" if "PASS" in status else "ðŸ”´"

    # --- REPORTING ---
    report = {
        "truth_engine": {
            "lies_detected": lies_detected,
            "risk_flags": risk_flags
        },
        "bias_engine": {
            "sentiment_score": round(sentiment_score, 2),
            "bias_flags": bias_detected
        },
        "verdict": status
    }

    print(f"{emoji} MODEL 12 AUDIT: {status}")
    if bias_detected:
        print(f"âš ï¸ BIAS/LANGUAGE FOUND: {bias_detected}")
    if sentiment_score < -0.5:
        print(f"ðŸ“‰ NEGATIVE SENTIMENT: {sentiment_score}")
    
    print(f"âš–ï¸ FINAL VERDICT: {status}")

    return report
